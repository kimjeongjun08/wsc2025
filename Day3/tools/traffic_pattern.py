# #!/usr/bin/env python3
# """
# AWS ALB íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„ íˆ´
# apdev-albì˜ product-tg, stress-tg, user-tg íƒ€ê²Ÿ ê·¸ë£¹ì—ì„œ request countë¥¼ ëª¨ë‹ˆí„°ë§
# """

# import boto3
# import time
# from datetime import datetime, timedelta
# import pytz
# from collections import defaultdict, deque
# import statistics
# import json
# from typing import Dict, List, Tuple
# import argparse
# import sys

# class TrafficPatternAnalyzer:
#     def __init__(self, load_balancer_name: str, region: str = 'ap-northeast-2'):
#         """
#         íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
#         Args:
#             load_balancer_name: ALB ì´ë¦„ (ì˜ˆ: apdev-alb)
#             region: AWS ë¦¬ì „
#         """
#         self.load_balancer_name = load_balancer_name
#         self.region = region
#         self.target_groups = ['product-tg', 'stress-tg', 'user-tg']
#         self.period_minutes = 1  # ê¸°ë³¸ ì§‘ê³„ ê¸°ê°„ (ë¶„) - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì— ìµœì í™”
        
#         # AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
#         try:
#             self.cloudwatch = boto3.client('cloudwatch', region_name=region)
#             self.elbv2 = boto3.client('elbv2', region_name=region)
#         except Exception as e:
#             print(f"âŒ AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
#             sys.exit(1)
        
#         # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
#         self.kst = pytz.timezone('Asia/Seoul')
        
#         # ë°ì´í„° ì €ì¥ìš©
#         self.traffic_data = defaultdict(deque)  # ê° íƒ€ê²Ÿ ê·¸ë£¹ë³„ íŠ¸ë˜í”½ ë°ì´í„°
#         self.history_length = 60  # ìµœê·¼ 60ê°œ ë°ì´í„° í¬ì¸íŠ¸ ìœ ì§€ (1ì‹œê°„ ë¶„ëŸ‰)
#         self.previous_values = {}  # ì´ì „ ê°’ ì €ì¥
#         self.pattern_thresholds = {
#             'spike': 30,      # ê¸‰ì¦ ì„ê³„ê°’ (% ì¦ê°€) - 1ë¶„ ë‹¨ìœ„ì— ë§ê²Œ ì¡°ì •
#             'drop': -25,      # ê¸‰ê° ì„ê³„ê°’ (% ê°ì†Œ)
#             'high_traffic': 50,   # ë†’ì€ íŠ¸ë˜í”½ ì„ê³„ê°’ (1ë¶„ë‹¹)
#         }
        
#     def get_alb_arn(self) -> str:
#         """ALB ARN ê°€ì ¸ì˜¤ê¸°"""
#         try:
#             response = self.elbv2.describe_load_balancers(
#                 Names=[self.load_balancer_name]
#             )
#             return response['LoadBalancers'][0]['LoadBalancerArn']
#         except Exception as e:
#             print(f"âŒ ALB '{self.load_balancer_name}' ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
#             return None
    
#     def get_target_group_metrics(self, target_group: str, period_minutes: int = 5) -> float:
#         """
#         íŠ¹ì • íƒ€ê²Ÿ ê·¸ë£¹ì˜ RequestCount ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°
        
#         Args:
#             target_group: íƒ€ê²Ÿ ê·¸ë£¹ ì´ë¦„
#             period_minutes: ì§‘ê³„ ê¸°ê°„ (ë¶„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 5ë¶„)
            
#         Returns:
#             ì§€ì •ëœ ê¸°ê°„ ë™ì•ˆì˜ RequestCount í•©ê³„
#         """
#         try:
#             # KST ì‹œê°„ì„ UTCë¡œ ë³€í™˜í•˜ì—¬ CloudWatchì— ì „ë‹¬
#             kst_now = datetime.now(self.kst)
#             end_time = kst_now.astimezone(pytz.UTC).replace(tzinfo=None)
#             start_time = end_time - timedelta(minutes=period_minutes)
            
#             response = self.cloudwatch.get_metric_statistics(
#                 Namespace='AWS/ApplicationELB',
#                 MetricName='RequestCount',
#                 Dimensions=[
#                     {
#                         'Name': 'LoadBalancer',
#                         'Value': self.load_balancer_name.replace('app/', '').split('/')[0]
#                     },
#                     {
#                         'Name': 'TargetGroup',
#                         'Value': target_group
#                     }
#                 ],
#                 StartTime=start_time,
#                 EndTime=end_time,
#                 Period=period_minutes * 60,  # ë¶„ì„ ì´ˆë¡œ ë³€í™˜
#                 Statistics=['Sum']  # RequestCount í•©ê³„
#             )
            
#             if response['Datapoints']:
#                 # ê°€ì¥ ìµœê·¼ ë°ì´í„°í¬ì¸íŠ¸ ë°˜í™˜
#                 latest = max(response['Datapoints'], key=lambda x: x['Timestamp'])
#                 return latest['Sum']
#             else:
#                 return 0.0
                
#         except Exception as e:
#             print(f"âš ï¸  {target_group} ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
#             return 0.0
    
#     def analyze_pattern(self, target_group: str, current_value: float) -> Dict:
#         """
#         íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„
        
#         Args:
#             target_group: íƒ€ê²Ÿ ê·¸ë£¹ ì´ë¦„
#             current_value: í˜„ì¬ RequestCount ê°’
            
#         Returns:
#             ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
#         """
#         analysis = {
#             'target_group': target_group,
#             'current_value': current_value,
#             'timestamp': datetime.now(self.kst),  # KST ì‹œê°„ ì‚¬ìš©
#             'patterns': [],
#             'trend': 'stable',
#             'change_percent': 0
#         }
        
#         # ì´ì „ ê°’ê³¼ ë¹„êµ
#         if target_group in self.previous_values:
#             prev_value = self.previous_values[target_group]
#             if prev_value > 0:
#                 change_percent = ((current_value - prev_value) / prev_value) * 100
#                 analysis['change_percent'] = change_percent
                
#                 # íŒ¨í„´ ê°ì§€
#                 if change_percent >= self.pattern_thresholds['spike']:
#                     analysis['patterns'].append('SPIKE')
#                     analysis['trend'] = 'increasing'
#                 elif change_percent <= self.pattern_thresholds['drop']:
#                     analysis['patterns'].append('DROP')
#                     analysis['trend'] = 'decreasing'
#                 elif change_percent > 10:
#                     analysis['trend'] = 'increasing'
#                 elif change_percent < -10:
#                     analysis['trend'] = 'decreasing'
        
#         # ë†’ì€ íŠ¸ë˜í”½ ê°ì§€
#         if current_value >= self.pattern_thresholds['high_traffic']:
#             analysis['patterns'].append('HIGH_TRAFFIC')
        
#         # ë°ì´í„° ì €ì¥
#         self.traffic_data[target_group].append({
#             'timestamp': analysis['timestamp'],
#             'value': current_value,
#             'change_percent': analysis['change_percent']
#         })
        
#         # ìµœëŒ€ ê¸¸ì´ ìœ ì§€
#         if len(self.traffic_data[target_group]) > self.history_length:
#             self.traffic_data[target_group].popleft()
        
#         # íŠ¸ë Œë“œ ë¶„ì„ (ìµœê·¼ 10ê°œ ë°ì´í„° í¬ì¸íŠ¸ ê¸°ì¤€)
#         if len(self.traffic_data[target_group]) >= 10:
#             recent_values = [item['value'] for item in list(self.traffic_data[target_group])[-10:]]
#             avg_change = statistics.mean([item['change_percent'] for item in list(self.traffic_data[target_group])[-5:]])
            
#             if avg_change > 10:
#                 analysis['patterns'].append('SUSTAINED_INCREASE')
#             elif avg_change < -10:
#                 analysis['patterns'].append('SUSTAINED_DECREASE')
        
#         # ì´ì „ ê°’ ì—…ë°ì´íŠ¸
#         self.previous_values[target_group] = current_value
        
#         return analysis
    
#     def format_analysis_output(self, analysis: Dict) -> str:
#         """ë¶„ì„ ê²°ê³¼ë¥¼ í¬ë§·ëœ ë¬¸ìì—´ë¡œ ë³€í™˜"""
#         timestamp = analysis['timestamp'].strftime('%H:%M:%S')
#         target_group = analysis['target_group']
#         current_value = analysis['current_value']
#         change_percent = analysis['change_percent']
#         patterns = analysis['patterns']
        
#         # ê¸°ë³¸ ì •ë³´
#         output = f"ğŸ¯ [{timestamp} KST] {target_group}: {current_value:.1f} requests"
        
#         # ë³€í™”ìœ¨ í‘œì‹œ
#         if change_percent != 0:
#             if change_percent > 0:
#                 output += f" (ğŸ”º +{change_percent:.1f}%)"
#             else:
#                 output += f" (ğŸ”» {change_percent:.1f}%)"
        
#         # íŒ¨í„´ í‘œì‹œ
#         if patterns:
#             pattern_emojis = {
#                 'SPIKE': 'ğŸš€',
#                 'DROP': 'ğŸ“‰',
#                 'HIGH_TRAFFIC': 'âš¡',
#                 'SUSTAINED_INCREASE': 'ğŸ“ˆ',
#                 'SUSTAINED_DECREASE': 'ğŸ“‰'
#             }
#             pattern_str = ' '.join([f"{pattern_emojis.get(p, 'âšª')} {p}" for p in patterns])
#             output += f" {pattern_str}"
        
#         return output
    
#     def run_continuous_analysis(self, interval: int = 60):
#         """
#         ì—°ì†ì ì¸ íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„ ì‹¤í–‰
        
#         Args:
#             interval: ëª¨ë‹ˆí„°ë§ ê°„ê²© (ì´ˆ)
#         """
#         print(f"ğŸš€ ALB '{self.load_balancer_name}' íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„ ì‹œì‘")
#         print(f"ğŸ“Š íƒ€ê²Ÿ ê·¸ë£¹: {', '.join(self.target_groups)}")
#         print(f"ğŸ“ˆ ë©”íŠ¸ë¦­ ì§‘ê³„ ê¸°ê°„: {self.period_minutes}ë¶„ (RequestCount í•©ê³„)")
#         print(f"â±ï¸  ëª¨ë‹ˆí„°ë§ ê°„ê²©: {interval}ì´ˆ")
#         print("=" * 80)
        
#         try:
#             while True:
#                 kst_now = datetime.now(self.kst)
#                 print(f"\nğŸ“… {kst_now.strftime('%Y-%m-%d %H:%M:%S KST')} - íŠ¸ë˜í”½ ë¶„ì„ ì¤‘...")
                
#                 for target_group in self.target_groups:
#                     # ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°
#                     request_count = self.get_target_group_metrics(target_group, self.period_minutes)
                    
#                     # íŒ¨í„´ ë¶„ì„
#                     analysis = self.analyze_pattern(target_group, request_count)
                    
#                     # ê²°ê³¼ ì¶œë ¥
#                     print(self.format_analysis_output(analysis))
                
#                 # ìš”ì•½ ì •ë³´
#                 total_requests = sum(self.previous_values.values())
#                 print(f"ğŸ“Š ì´ ìš”ì²­ ìˆ˜: {total_requests:.1f} requests")
#                 print("-" * 80)
                
#                 # ëŒ€ê¸°
#                 time.sleep(interval)
                
#         except KeyboardInterrupt:
#             print("\n\nâ¹ï¸  ëª¨ë‹ˆí„°ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
#             self.print_summary()
#         except Exception as e:
#             print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
#     def print_summary(self):
#         """íŠ¸ë˜í”½ ë¶„ì„ ìš”ì•½ ì¶œë ¥"""
#         print("\n" + "="*80)
#         print("ğŸ“Š íŠ¸ë˜í”½ ë¶„ì„ ìš”ì•½")
#         print("="*80)
        
#         for target_group in self.target_groups:
#             if target_group in self.traffic_data:
#                 data = list(self.traffic_data[target_group])
#                 if data:
#                     values = [item['value'] for item in data]
#                     avg_value = statistics.mean(values)
#                     max_value = max(values)
#                     min_value = min(values)
                    
#                     print(f"\nğŸ¯ {target_group}:")
#                     print(f"   í‰ê· : {avg_value:.1f} requests")
#                     print(f"   ìµœëŒ€: {max_value:.1f} requests")
#                     print(f"   ìµœì†Œ: {min_value:.1f} requests")

# def main():
#     parser = argparse.ArgumentParser(description='AWS ALB íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„ íˆ´')
#     parser.add_argument('--alb-name', default='apdev-alb', 
#                        help='ALB ì´ë¦„ (ê¸°ë³¸ê°’: apdev-alb)')
#     parser.add_argument('--region', default='ap-northeast-2',
#                        help='AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)')
#     parser.add_argument('--interval', type=int, default=60,
#                        help='ëª¨ë‹ˆí„°ë§ ê°„ê²© (ì´ˆ, ê¸°ë³¸ê°’: 60)')
#     parser.add_argument('--period', type=int, default=1,
#                        help='ë©”íŠ¸ë¦­ ì§‘ê³„ ê¸°ê°„ (ë¶„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 1ë¶„ - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì— ìµœì í™”)')
    
#     args = parser.parse_args()
    
#     # íŠ¸ë˜í”½ ë¶„ì„ê¸° ìƒì„± ë° ì‹¤í–‰
#     analyzer = TrafficPatternAnalyzer(args.alb_name, args.region)
#     analyzer.period_minutes = args.period  # ì§‘ê³„ ê¸°ê°„ ì„¤ì •
#     analyzer.run_continuous_analysis(args.interval)

# if __name__ == "__main__":
#     main()
#!/usr/bin/env python3
"""
AWS ALB íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„ íˆ´ - ë””ë²„ê·¸ ë²„ì „
apdev-albì˜ product-tg, stress-tg, user-tg íƒ€ê²Ÿ ê·¸ë£¹ì—ì„œ request countë¥¼ ëª¨ë‹ˆí„°ë§
"""

import boto3
import time
from datetime import datetime, timedelta
import pytz
from collections import defaultdict, deque
import statistics
import json
from typing import Dict, List, Tuple
import argparse
import sys

class TrafficPatternAnalyzer:
    def __init__(self, load_balancer_name: str, region: str = 'ap-northeast-2'):
        """
        íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            load_balancer_name: ALB ì´ë¦„ (ì˜ˆ: apdev-alb)
            region: AWS ë¦¬ì „
        """
        self.load_balancer_name = load_balancer_name
        self.region = region
        self.target_groups = ['product-tg', 'stress-tg', 'user-tg']
        self.period_minutes = 1
        
        # AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.cloudwatch = boto3.client('cloudwatch', region_name=region)
            self.elbv2 = boto3.client('elbv2', region_name=region)
        except Exception as e:
            print(f"âŒ AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            sys.exit(1)
        
        # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
        self.kst = pytz.timezone('Asia/Seoul')
        
        # ë°ì´í„° ì €ì¥ìš©
        self.traffic_data = defaultdict(deque)
        self.history_length = 60
        self.previous_values = {}
        self.pattern_thresholds = {
            'spike': 30,
            'drop': -25,
            'high_traffic': 50,
        }
        
        # ALBì™€ íƒ€ê²Ÿ ê·¸ë£¹ ì •ë³´ ì´ˆê¸°í™”
        self.alb_arn = None
        self.target_group_arns = {}
        self.initialize_aws_resources()
        
    def initialize_aws_resources(self):
        """ALBì™€ íƒ€ê²Ÿ ê·¸ë£¹ ì •ë³´ ì´ˆê¸°í™” ë° ê²€ì¦"""
        print("ğŸ” AWS ë¦¬ì†ŒìŠ¤ ì •ë³´ í™•ì¸ ì¤‘...")
        
        # ALB ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        try:
            response = self.elbv2.describe_load_balancers()
            alb_found = False
            
            print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ALB ëª©ë¡:")
            for lb in response['LoadBalancers']:
                lb_name = lb['LoadBalancerName']
                print(f"  - {lb_name}")
                if lb_name == self.load_balancer_name:
                    self.alb_arn = lb['LoadBalancerArn']
                    alb_found = True
                    print(f"    âœ… ëŒ€ìƒ ALB ë°œê²¬: {lb_name}")
            
            if not alb_found:
                print(f"âŒ ALB '{self.load_balancer_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            print(f"âŒ ALB ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return False
        
        # íƒ€ê²Ÿ ê·¸ë£¹ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        try:
            response = self.elbv2.describe_target_groups()
            found_target_groups = []
            
            print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ íƒ€ê²Ÿ ê·¸ë£¹ ëª©ë¡:")
            for tg in response['TargetGroups']:
                tg_name = tg['TargetGroupName']
                print(f"  - {tg_name}")
                if tg_name in self.target_groups:
                    self.target_group_arns[tg_name] = tg['TargetGroupArn']
                    found_target_groups.append(tg_name)
                    print(f"    âœ… ëŒ€ìƒ íƒ€ê²Ÿ ê·¸ë£¹ ë°œê²¬: {tg_name}")
            
            missing_tgs = set(self.target_groups) - set(found_target_groups)
            if missing_tgs:
                print(f"âš ï¸  ë‹¤ìŒ íƒ€ê²Ÿ ê·¸ë£¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {missing_tgs}")
                self.target_groups = found_target_groups
                print(f"ğŸ“Š ë¶„ì„í•  íƒ€ê²Ÿ ê·¸ë£¹: {self.target_groups}")
                
        except Exception as e:
            print(f"âŒ íƒ€ê²Ÿ ê·¸ë£¹ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return False
        
        return True
    
    def get_correct_dimension_values(self, target_group: str) -> Tuple[str, str]:
        """ì˜¬ë°”ë¥¸ Dimension ê°’ ë°˜í™˜"""
        # ALB ARNì—ì„œ LoadBalancer dimension ê°’ ì¶”ì¶œ
        # arn:aws:elasticloadbalancing:region:account:loadbalancer/app/name/id
        # -> app/name/id í˜•íƒœë¡œ ë³€í™˜
        alb_dimension = self.alb_arn.split('/')[-3:]  # ['app', 'name', 'id']
        alb_dimension_value = '/'.join(alb_dimension)
        
        # íƒ€ê²Ÿ ê·¸ë£¹ ARNì—ì„œ TargetGroup dimension ê°’ ì¶”ì¶œ
        # arn:aws:elasticloadbalancing:region:account:targetgroup/name/id
        # -> targetgroup/name/id í˜•íƒœë¡œ ë³€í™˜
        tg_arn = self.target_group_arns[target_group]
        tg_dimension = tg_arn.split('/')[-3:]  # ['targetgroup', 'name', 'id']
        tg_dimension_value = '/'.join(tg_dimension)
        
        return alb_dimension_value, tg_dimension_value
    
    def debug_cloudwatch_query(self, target_group: str, period_minutes: int = 5):
        """CloudWatch ì¿¼ë¦¬ ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥"""
        try:
            alb_dim, tg_dim = self.get_correct_dimension_values(target_group)
            
            kst_now = datetime.now(self.kst)
            end_time = kst_now.astimezone(pytz.UTC).replace(tzinfo=None)
            start_time = end_time - timedelta(minutes=period_minutes)
            
            print(f"\nğŸ” CloudWatch ì¿¼ë¦¬ ë””ë²„ê·¸ - {target_group}:")
            print(f"  ğŸ“… ì‹œê°„ ë²”ìœ„: {start_time} ~ {end_time} UTC")
            print(f"  ğŸ“Š Namespace: AWS/ApplicationELB")
            print(f"  ğŸ“ MetricName: RequestCount")
            print(f"  ğŸ¯ LoadBalancer Dimension: {alb_dim}")
            print(f"  ğŸ¯ TargetGroup Dimension: {tg_dim}")
            print(f"  â±ï¸  Period: {period_minutes * 60}ì´ˆ")
            
            # ì‹¤ì œ ì¿¼ë¦¬ ì‹¤í–‰
            response = self.cloudwatch.get_metric_statistics(
                Namespace='AWS/ApplicationELB',
                MetricName='RequestCount',
                Dimensions=[
                    {
                        'Name': 'LoadBalancer',
                        'Value': alb_dim
                    },
                    {
                        'Name': 'TargetGroup',
                        'Value': tg_dim
                    }
                ],
                StartTime=start_time,
                EndTime=end_time,
                Period=period_minutes * 60,
                Statistics=['Sum']
            )
            
            print(f"  ğŸ“ˆ ë°˜í™˜ëœ ë°ì´í„°í¬ì¸íŠ¸ ìˆ˜: {len(response['Datapoints'])}")
            if response['Datapoints']:
                for dp in response['Datapoints']:
                    print(f"    - {dp['Timestamp']}: {dp['Sum']} requests")
            else:
                print("    âš ï¸  ë°ì´í„°í¬ì¸íŠ¸ ì—†ìŒ")
                
                # ALB ì „ì²´ ë ˆë²¨ì—ì„œ ë©”íŠ¸ë¦­ í™•ì¸
                print(f"  ğŸ”„ ALB ì „ì²´ ë ˆë²¨ ë©”íŠ¸ë¦­ í™•ì¸...")
                alb_only_response = self.cloudwatch.get_metric_statistics(
                    Namespace='AWS/ApplicationELB',
                    MetricName='RequestCount',
                    Dimensions=[
                        {
                            'Name': 'LoadBalancer',
                            'Value': alb_dim
                        }
                    ],
                    StartTime=end_time - timedelta(minutes=30),
                    EndTime=end_time,
                    Period=300,
                    Statistics=['Sum']
                )
                print(f"    ğŸ“Š ALB ì „ì²´ 30ë¶„ê°„ ë°ì´í„°í¬ì¸íŠ¸ ìˆ˜: {len(alb_only_response['Datapoints'])}")
                if alb_only_response['Datapoints']:
                    for dp in alb_only_response['Datapoints']:
                        print(f"      - {dp['Timestamp']}: {dp['Sum']} requests (ì „ì²´ ALB)")
                
                # ë‹¤ë¥¸ ê°€ëŠ¥í•œ dimension ì¡°í•© ì‹œë„
                print(f"  ğŸ”„ íƒ€ê²Ÿ ê·¸ë£¹ë§Œìœ¼ë¡œ ë©”íŠ¸ë¦­ í™•ì¸...")
                tg_only_response = self.cloudwatch.get_metric_statistics(
                    Namespace='AWS/ApplicationELB',
                    MetricName='RequestCount',
                    Dimensions=[
                        {
                            'Name': 'TargetGroup',
                            'Value': tg_dim
                        }
                    ],
                    StartTime=end_time - timedelta(minutes=30),
                    EndTime=end_time,
                    Period=300,
                    Statistics=['Sum']
                )
                print(f"    ğŸ“Š íƒ€ê²Ÿ ê·¸ë£¹ë§Œ 30ë¶„ê°„ ë°ì´í„°í¬ì¸íŠ¸ ìˆ˜: {len(tg_only_response['Datapoints'])}")
                if tg_only_response['Datapoints']:
                    for dp in tg_only_response['Datapoints']:
                        print(f"      - {dp['Timestamp']}: {dp['Sum']} requests (íƒ€ê²Ÿ ê·¸ë£¹ë§Œ)")
                
                # ì˜ëª»ëœ dimension ê°’ìœ¼ë¡œ ì‹œë„ (ì›ë˜ ì½”ë“œ ë°©ì‹)
                print(f"  ğŸ”„ ì›ë˜ dimension í˜•ì‹ìœ¼ë¡œ ì‹œë„...")
                original_response = self.cloudwatch.get_metric_statistics(
                    Namespace='AWS/ApplicationELB',
                    MetricName='RequestCount',
                    Dimensions=[
                        {
                            'Name': 'LoadBalancer',
                            'Value': self.load_balancer_name.replace('app/', '').split('/')[0]
                        },
                        {
                            'Name': 'TargetGroup',
                            'Value': target_group
                        }
                    ],
                    StartTime=end_time - timedelta(minutes=30),
                    EndTime=end_time,
                    Period=300,
                    Statistics=['Sum']
                )
                print(f"    ğŸ“Š ì›ë˜ ë°©ì‹ 30ë¶„ê°„ ë°ì´í„°í¬ì¸íŠ¸ ìˆ˜: {len(original_response['Datapoints'])}")
                if original_response['Datapoints']:
                    for dp in original_response['Datapoints']:
                        print(f"      - {dp['Timestamp']}: {dp['Sum']} requests (ì›ë˜ ë°©ì‹)")
            
            return response
            
        except Exception as e:
            print(f"âŒ CloudWatch ì¿¼ë¦¬ ë””ë²„ê·¸ ì‹¤íŒ¨: {e}")
            return None
    
    def get_target_group_metrics(self, target_group: str, period_minutes: int = 5) -> float:
        """íŠ¹ì • íƒ€ê²Ÿ ê·¸ë£¹ì˜ RequestCount ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°"""
        try:
            alb_dim, tg_dim = self.get_correct_dimension_values(target_group)
            
            kst_now = datetime.now(self.kst)
            end_time = kst_now.astimezone(pytz.UTC).replace(tzinfo=None)
            start_time = end_time - timedelta(minutes=period_minutes)
            
            response = self.cloudwatch.get_metric_statistics(
                Namespace='AWS/ApplicationELB',
                MetricName='RequestCount',
                Dimensions=[
                    {
                        'Name': 'LoadBalancer',
                        'Value': alb_dim
                    },
                    {
                        'Name': 'TargetGroup',
                        'Value': tg_dim
                    }
                ],
                StartTime=start_time,
                EndTime=end_time,
                Period=period_minutes * 60,
                Statistics=['Sum']
            )
            
            if response['Datapoints']:
                latest = max(response['Datapoints'], key=lambda x: x['Timestamp'])
                return latest['Sum']
            else:
                return 0.0
                
        except Exception as e:
            print(f"âš ï¸  {target_group} ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def analyze_pattern(self, target_group: str, current_value: float) -> Dict:
        """íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„"""
        analysis = {
            'target_group': target_group,
            'current_value': current_value,
            'timestamp': datetime.now(self.kst),
            'patterns': [],
            'trend': 'stable',
            'change_percent': 0
        }
        
        if target_group in self.previous_values:
            prev_value = self.previous_values[target_group]
            if prev_value > 0:
                change_percent = ((current_value - prev_value) / prev_value) * 100
                analysis['change_percent'] = change_percent
                
                if change_percent >= self.pattern_thresholds['spike']:
                    analysis['patterns'].append('SPIKE')
                    analysis['trend'] = 'increasing'
                elif change_percent <= self.pattern_thresholds['drop']:
                    analysis['patterns'].append('DROP')
                    analysis['trend'] = 'decreasing'
                elif change_percent > 10:
                    analysis['trend'] = 'increasing'
                elif change_percent < -10:
                    analysis['trend'] = 'decreasing'
        
        if current_value >= self.pattern_thresholds['high_traffic']:
            analysis['patterns'].append('HIGH_TRAFFIC')
        
        self.traffic_data[target_group].append({
            'timestamp': analysis['timestamp'],
            'value': current_value,
            'change_percent': analysis['change_percent']
        })
        
        if len(self.traffic_data[target_group]) > self.history_length:
            self.traffic_data[target_group].popleft()
        
        if len(self.traffic_data[target_group]) >= 10:
            avg_change = statistics.mean([item['change_percent'] for item in list(self.traffic_data[target_group])[-5:]])
            
            if avg_change > 10:
                analysis['patterns'].append('SUSTAINED_INCREASE')
            elif avg_change < -10:
                analysis['patterns'].append('SUSTAINED_DECREASE')
        
        self.previous_values[target_group] = current_value
        return analysis
    
    def format_analysis_output(self, analysis: Dict) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ í¬ë§·ëœ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        timestamp = analysis['timestamp'].strftime('%H:%M:%S')
        target_group = analysis['target_group']
        current_value = analysis['current_value']
        change_percent = analysis['change_percent']
        patterns = analysis['patterns']
        
        output = f"ğŸ¯ [{timestamp} KST] {target_group}: {current_value:.1f} requests"
        
        if change_percent != 0:
            if change_percent > 0:
                output += f" (ğŸ”º +{change_percent:.1f}%)"
            else:
                output += f" (ğŸ”» {change_percent:.1f}%)"
        
        if patterns:
            pattern_emojis = {
                'SPIKE': 'ğŸš€',
                'DROP': 'ğŸ“‰',
                'HIGH_TRAFFIC': 'âš¡',
                'SUSTAINED_INCREASE': 'ğŸ“ˆ',
                'SUSTAINED_DECREASE': 'ğŸ“‰'
            }
            pattern_str = ' '.join([f"{pattern_emojis.get(p, 'âšª')} {p}" for p in patterns])
            output += f" {pattern_str}"
        
        return output
    
    def run_debug_mode(self):
        """ë””ë²„ê·¸ ëª¨ë“œ ì‹¤í–‰ - í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ê³  ìƒì„¸ ì •ë³´ ì¶œë ¥"""
        print("ğŸ› ë””ë²„ê·¸ ëª¨ë“œ ì‹¤í–‰")
        print("=" * 80)
        
        for target_group in self.target_groups:
            self.debug_cloudwatch_query(target_group, self.period_minutes)
            print("-" * 40)
    
    def run_continuous_analysis(self, interval: int = 60):
        """ì—°ì†ì ì¸ íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„ ì‹¤í–‰"""
        print(f"ğŸš€ ALB '{self.load_balancer_name}' íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„ ì‹œì‘")
        print(f"ğŸ“Š íƒ€ê²Ÿ ê·¸ë£¹: {', '.join(self.target_groups)}")
        print(f"ğŸ“ˆ ë©”íŠ¸ë¦­ ì§‘ê³„ ê¸°ê°„: {self.period_minutes}ë¶„")
        print(f"â±ï¸  ëª¨ë‹ˆí„°ë§ ê°„ê²©: {interval}ì´ˆ")
        print("=" * 80)
        
        try:
            while True:
                kst_now = datetime.now(self.kst)
                print(f"\nğŸ“… {kst_now.strftime('%Y-%m-%d %H:%M:%S KST')} - íŠ¸ë˜í”½ ë¶„ì„ ì¤‘...")
                
                for target_group in self.target_groups:
                    request_count = self.get_target_group_metrics(target_group, self.period_minutes)
                    analysis = self.analyze_pattern(target_group, request_count)
                    print(self.format_analysis_output(analysis))
                
                total_requests = sum(self.previous_values.values())
                print(f"ğŸ“Š ì´ ìš”ì²­ ìˆ˜: {total_requests:.1f} requests")
                print("-" * 80)
                
                time.sleep(interval)
                
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  ëª¨ë‹ˆí„°ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            self.print_summary()
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def print_summary(self):
        """íŠ¸ë˜í”½ ë¶„ì„ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š íŠ¸ë˜í”½ ë¶„ì„ ìš”ì•½")
        print("="*80)
        
        for target_group in self.target_groups:
            if target_group in self.traffic_data:
                data = list(self.traffic_data[target_group])
                if data:
                    values = [item['value'] for item in data]
                    avg_value = statistics.mean(values)
                    max_value = max(values)
                    min_value = min(values)
                    
                    print(f"\nğŸ¯ {target_group}:")
                    print(f"   í‰ê· : {avg_value:.1f} requests")
                    print(f"   ìµœëŒ€: {max_value:.1f} requests")
                    print(f"   ìµœì†Œ: {min_value:.1f} requests")

def main():
    parser = argparse.ArgumentParser(description='AWS ALB íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„ íˆ´')
    parser.add_argument('--alb-name', default='apdev-alb', 
                       help='ALB ì´ë¦„ (ê¸°ë³¸ê°’: apdev-alb)')
    parser.add_argument('--region', default='ap-northeast-2',
                       help='AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)')
    parser.add_argument('--interval', type=int, default=60,
                       help='ëª¨ë‹ˆí„°ë§ ê°„ê²© (ì´ˆ, ê¸°ë³¸ê°’: 60)')
    parser.add_argument('--period', type=int, default=5,
                       help='ë©”íŠ¸ë¦­ ì§‘ê³„ ê¸°ê°„ (ë¶„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 5ë¶„)')
    parser.add_argument('--debug', action='store_true',
                       help='ë””ë²„ê·¸ ëª¨ë“œ ì‹¤í–‰ (í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ê³  ìƒì„¸ ì •ë³´ ì¶œë ¥)')
    
    args = parser.parse_args()
    
    analyzer = TrafficPatternAnalyzer(args.alb_name, args.region)
    analyzer.period_minutes = args.period
    
    if args.debug:
        analyzer.run_debug_mode()
    else:
        analyzer.run_continuous_analysis(args.interval)

if __name__ == "__main__":
    main()